{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import keras\n",
    "board = keras.callbacks.TensorBoard(log_dir='./logs/artsnobs', histogram_freq=0, batch_size=100, write_graph=True)\n",
    "\n",
    "#Dynamically change learning rate based on 'val_acc' over time\n",
    "#change 0.8 to 0.75?\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.75, patience=7, min_lr=0.001, verbose=1)\n",
    "\n",
    "#Stop training when 'val_loss' stops improving after 15 epochs\n",
    "#patience was 15\n",
    "e_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "\n",
    "#Next 4 lines allows MASSIVE images to be loaded in and used\n",
    "import PIL\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "PIL.Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3)))\n",
    "# model.add(Conv2D(32, (3, 3), input_shape=(175, 175, 3)))\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(500, 500, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "# model.add(BatchNormalization()) #added this\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3))) #changed 32 to 64\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3))) #added this\n",
    "model.add(Activation('relu')) #added this\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) #added this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# model.add(Dense(64)) #added this\n",
    "# model.add(Activation('sigmoid')) #added this\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3)) #3 for art_genre_set, 5 for dataset\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adadelta', metrics = ['accuracy'])\n",
    "# model.summary()\n",
    "model.load_weights('artsnobs_weights.h5') #BE AWARE IF THIS IS ENABLED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 #change this from 16, used 100 for dataset\n",
    "# train_datagen = ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255, rotation_range=40, width_shift_range=0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True, fill_mode = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19011 images belonging to 3 classes.\n",
      "Found 2112 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "train_generator = train_datagen.flow_from_directory('/Users/ethan/Documents/MLData/ArtSnobs/art_genre_set/train/', target_size = (500, 500), batch_size = batch_size, class_mode = 'categorical', color_mode='rgb')\n",
    "v_generator = test_datagen.flow_from_directory('/Users/ethan/Documents/MLData/ArtSnobs/art_genre_set/test/', target_size = (500, 500), batch_size = batch_size, class_mode = 'categorical', color_mode='rgb')\n",
    "\n",
    "# train_generator = train_datagen.flow_from_directory('/Users/ethan/Documents/MLData/ArtSnobs/dataset/dataset_updated/training_set/', target_size = (175, 175), batch_size = batch_size, class_mode = 'categorical', color_mode='rgb')\n",
    "# v_generator = test_datagen.flow_from_directory('/Users/ethan/Documents/MLData/ArtSnobs/dataset/dataset_updated/validation_set/', target_size = (175,175), batch_size = batch_size, class_mode = 'categorical', color_mode='rgb')\n",
    "# # train_generator = train_datagen.flow_from_directory('/Users/ethan/Documents/MLData/ArtSnobs/dataset/dataset_updated/training_set/', target_size = (150, 150), batch_size = batch_size, class_mode = 'categorical', color_mode='rgb')\n",
    "# # v_generator = test_datagen.flow_from_directory('/Users/ethan/Documents/MLData/ArtSnobs/dataset/dataset_updated/validation_set/', target_size = (150,150), batch_size = batch_size, class_mode = 'categorical', color_mode='rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/250\n",
      "156/156 [==============================] - 54s 348ms/step - loss: 1.0981 - acc: 0.3782 - val_loss: 1.0806 - val_acc: 0.4311\n",
      "Epoch 2/250\n",
      "156/156 [==============================] - 61s 391ms/step - loss: 1.0782 - acc: 0.4123 - val_loss: 1.0805 - val_acc: 0.4111\n",
      "Epoch 3/250\n",
      "156/156 [==============================] - 50s 323ms/step - loss: 1.0767 - acc: 0.4227 - val_loss: 1.0715 - val_acc: 0.4191\n",
      "Epoch 4/250\n",
      "156/156 [==============================] - 52s 334ms/step - loss: 1.0661 - acc: 0.4247 - val_loss: 1.0773 - val_acc: 0.4119\n",
      "Epoch 5/250\n",
      "156/156 [==============================] - 52s 331ms/step - loss: 1.0657 - acc: 0.4419 - val_loss: 1.0581 - val_acc: 0.4359\n",
      "Epoch 6/250\n",
      "156/156 [==============================] - 54s 349ms/step - loss: 1.0578 - acc: 0.4283 - val_loss: 1.0527 - val_acc: 0.4495\n",
      "Epoch 7/250\n",
      "156/156 [==============================] - 51s 326ms/step - loss: 1.0471 - acc: 0.4527 - val_loss: 1.0594 - val_acc: 0.4415\n",
      "Epoch 8/250\n",
      "156/156 [==============================] - 50s 319ms/step - loss: 1.0510 - acc: 0.4497 - val_loss: 1.0783 - val_acc: 0.4191\n",
      "Epoch 9/250\n",
      "156/156 [==============================] - 51s 329ms/step - loss: 1.0393 - acc: 0.4567 - val_loss: 1.0355 - val_acc: 0.4599\n",
      "Epoch 10/250\n",
      "156/156 [==============================] - 51s 329ms/step - loss: 1.0418 - acc: 0.4683 - val_loss: 1.0345 - val_acc: 0.4487\n",
      "Epoch 11/250\n",
      "156/156 [==============================] - 59s 376ms/step - loss: 1.0366 - acc: 0.4756 - val_loss: 1.0180 - val_acc: 0.4583\n",
      "Epoch 12/250\n",
      "156/156 [==============================] - 52s 335ms/step - loss: 1.0314 - acc: 0.4692 - val_loss: 1.0445 - val_acc: 0.4455\n",
      "Epoch 13/250\n",
      "156/156 [==============================] - 51s 328ms/step - loss: 1.0270 - acc: 0.4748 - val_loss: 1.0140 - val_acc: 0.4744\n",
      "Epoch 14/250\n",
      "156/156 [==============================] - 54s 345ms/step - loss: 1.0319 - acc: 0.4611 - val_loss: 1.0415 - val_acc: 0.4399\n",
      "Epoch 15/250\n",
      "156/156 [==============================] - 54s 345ms/step - loss: 1.0175 - acc: 0.4824 - val_loss: 1.0221 - val_acc: 0.4792\n",
      "Epoch 16/250\n",
      "156/156 [==============================] - 53s 337ms/step - loss: 1.0110 - acc: 0.4992 - val_loss: 1.0094 - val_acc: 0.4800\n",
      "Epoch 17/250\n",
      "156/156 [==============================] - 50s 323ms/step - loss: 1.0319 - acc: 0.4712 - val_loss: 1.1174 - val_acc: 0.4439\n",
      "Epoch 18/250\n",
      "156/156 [==============================] - 61s 393ms/step - loss: 1.0120 - acc: 0.4792 - val_loss: 1.0071 - val_acc: 0.4591\n",
      "Epoch 19/250\n",
      "156/156 [==============================] - 51s 327ms/step - loss: 1.0102 - acc: 0.4924 - val_loss: 0.9896 - val_acc: 0.5096\n",
      "Epoch 20/250\n",
      "156/156 [==============================] - 52s 332ms/step - loss: 1.0101 - acc: 0.4916 - val_loss: 0.9927 - val_acc: 0.4784\n",
      "Epoch 21/250\n",
      "156/156 [==============================] - 51s 327ms/step - loss: 1.0004 - acc: 0.4808 - val_loss: 0.9936 - val_acc: 0.5000\n",
      "Epoch 22/250\n",
      "156/156 [==============================] - 52s 332ms/step - loss: 0.9968 - acc: 0.5004 - val_loss: 0.9969 - val_acc: 0.4888\n",
      "Epoch 23/250\n",
      "156/156 [==============================] - 53s 337ms/step - loss: 0.9874 - acc: 0.5045 - val_loss: 1.0218 - val_acc: 0.4984\n",
      "Epoch 24/250\n",
      "156/156 [==============================] - 50s 323ms/step - loss: 0.9931 - acc: 0.5104 - val_loss: 1.0131 - val_acc: 0.4872\n",
      "Epoch 25/250\n",
      "156/156 [==============================] - 53s 342ms/step - loss: 0.9810 - acc: 0.4880 - val_loss: 0.9773 - val_acc: 0.5128\n",
      "Epoch 26/250\n",
      "156/156 [==============================] - 54s 346ms/step - loss: 0.9833 - acc: 0.5048 - val_loss: 1.0080 - val_acc: 0.4696\n",
      "Epoch 27/250\n",
      "156/156 [==============================] - 50s 323ms/step - loss: 0.9845 - acc: 0.5084 - val_loss: 0.9886 - val_acc: 0.5040\n",
      "Epoch 28/250\n",
      "156/156 [==============================] - 51s 324ms/step - loss: 0.9698 - acc: 0.5312 - val_loss: 0.9942 - val_acc: 0.4880\n",
      "Epoch 29/250\n",
      "156/156 [==============================] - 52s 334ms/step - loss: 0.9918 - acc: 0.4996 - val_loss: 0.9817 - val_acc: 0.5064\n",
      "Epoch 30/250\n",
      "156/156 [==============================] - 60s 383ms/step - loss: 0.9960 - acc: 0.5000 - val_loss: 0.9696 - val_acc: 0.5176\n",
      "Epoch 31/250\n",
      "156/156 [==============================] - 52s 334ms/step - loss: 0.9776 - acc: 0.5027 - val_loss: 0.9919 - val_acc: 0.5048\n",
      "Epoch 32/250\n",
      "156/156 [==============================] - 55s 354ms/step - loss: 0.9619 - acc: 0.5196 - val_loss: 0.9992 - val_acc: 0.4936\n",
      "Epoch 33/250\n",
      "156/156 [==============================] - 52s 331ms/step - loss: 0.9582 - acc: 0.5321 - val_loss: 1.0176 - val_acc: 0.4792\n",
      "Epoch 34/250\n",
      "156/156 [==============================] - 58s 372ms/step - loss: 0.9850 - acc: 0.5036 - val_loss: 0.9666 - val_acc: 0.4992\n",
      "Epoch 35/250\n",
      "156/156 [==============================] - 53s 341ms/step - loss: 0.9739 - acc: 0.5284 - val_loss: 0.9649 - val_acc: 0.5296\n",
      "Epoch 36/250\n",
      "156/156 [==============================] - 52s 335ms/step - loss: 0.9610 - acc: 0.5132 - val_loss: 0.9877 - val_acc: 0.5016\n",
      "Epoch 37/250\n",
      "156/156 [==============================] - 51s 330ms/step - loss: 0.9591 - acc: 0.5433 - val_loss: 0.9276 - val_acc: 0.5409\n",
      "Epoch 38/250\n",
      "156/156 [==============================] - 56s 360ms/step - loss: 0.9523 - acc: 0.5337 - val_loss: 0.9417 - val_acc: 0.5337\n",
      "Epoch 39/250\n",
      "156/156 [==============================] - 52s 333ms/step - loss: 0.9389 - acc: 0.5423 - val_loss: 0.9601 - val_acc: 0.5280\n",
      "Epoch 40/250\n",
      "156/156 [==============================] - 53s 339ms/step - loss: 0.9553 - acc: 0.5264 - val_loss: 0.9526 - val_acc: 0.5441\n",
      "Epoch 41/250\n",
      "156/156 [==============================] - 63s 405ms/step - loss: 0.9585 - acc: 0.5329 - val_loss: 0.9545 - val_acc: 0.5232\n",
      "Epoch 42/250\n",
      "156/156 [==============================] - 55s 352ms/step - loss: 0.9624 - acc: 0.5453 - val_loss: 0.9315 - val_acc: 0.5513\n",
      "Epoch 43/250\n",
      "156/156 [==============================] - 52s 330ms/step - loss: 0.9394 - acc: 0.5537 - val_loss: 0.9742 - val_acc: 0.5024\n",
      "Epoch 44/250\n",
      "156/156 [==============================] - 54s 349ms/step - loss: 0.9403 - acc: 0.5421 - val_loss: 0.9505 - val_acc: 0.5288\n",
      "Epoch 45/250\n",
      "156/156 [==============================] - 55s 351ms/step - loss: 0.9474 - acc: 0.5349 - val_loss: 0.9571 - val_acc: 0.5104\n",
      "Epoch 46/250\n",
      "156/156 [==============================] - 55s 354ms/step - loss: 0.9267 - acc: 0.5429 - val_loss: 0.9234 - val_acc: 0.5585\n",
      "Epoch 47/250\n",
      "156/156 [==============================] - 54s 345ms/step - loss: 0.9570 - acc: 0.5345 - val_loss: 0.9546 - val_acc: 0.5112\n",
      "Epoch 48/250\n",
      "156/156 [==============================] - 55s 356ms/step - loss: 0.9463 - acc: 0.5425 - val_loss: 0.9181 - val_acc: 0.5425\n",
      "Epoch 49/250\n",
      "156/156 [==============================] - 52s 335ms/step - loss: 0.9331 - acc: 0.5417 - val_loss: 1.0923 - val_acc: 0.4832\n",
      "Epoch 50/250\n",
      "156/156 [==============================] - 54s 343ms/step - loss: 0.9521 - acc: 0.5569 - val_loss: 0.9614 - val_acc: 0.5369\n",
      "Epoch 51/250\n",
      "156/156 [==============================] - 54s 348ms/step - loss: 0.9318 - acc: 0.5669 - val_loss: 0.9346 - val_acc: 0.5441\n",
      "Epoch 52/250\n",
      "156/156 [==============================] - 63s 406ms/step - loss: 0.9386 - acc: 0.5585 - val_loss: 0.9660 - val_acc: 0.5152\n",
      "Epoch 53/250\n",
      "156/156 [==============================] - 54s 344ms/step - loss: 0.9302 - acc: 0.5653 - val_loss: 0.9371 - val_acc: 0.5144\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.75.\n",
      "Epoch 54/250\n",
      "156/156 [==============================] - 51s 327ms/step - loss: 0.9239 - acc: 0.5733 - val_loss: 0.9406 - val_acc: 0.5312\n",
      "Epoch 55/250\n",
      "156/156 [==============================] - 52s 332ms/step - loss: 0.9189 - acc: 0.5721 - val_loss: 0.9188 - val_acc: 0.5321\n",
      "Epoch 56/250\n",
      "156/156 [==============================] - 53s 337ms/step - loss: 0.9061 - acc: 0.5725 - val_loss: 0.9215 - val_acc: 0.5489\n",
      "Epoch 57/250\n",
      "156/156 [==============================] - 54s 345ms/step - loss: 0.9262 - acc: 0.5513 - val_loss: 0.9386 - val_acc: 0.5457\n",
      "Epoch 58/250\n",
      "156/156 [==============================] - 54s 343ms/step - loss: 0.9075 - acc: 0.5717 - val_loss: 0.9177 - val_acc: 0.5441\n",
      "Epoch 59/250\n",
      "156/156 [==============================] - 54s 346ms/step - loss: 0.9071 - acc: 0.5777 - val_loss: 0.9399 - val_acc: 0.5433\n",
      "Epoch 60/250\n",
      "156/156 [==============================] - 54s 344ms/step - loss: 0.9185 - acc: 0.5609 - val_loss: 0.9221 - val_acc: 0.5312\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.5625.\n",
      "Epoch 61/250\n",
      "156/156 [==============================] - 60s 385ms/step - loss: 0.9111 - acc: 0.5767 - val_loss: 0.8982 - val_acc: 0.5745\n",
      "Epoch 62/250\n",
      "156/156 [==============================] - 52s 333ms/step - loss: 0.8888 - acc: 0.5905 - val_loss: 0.9154 - val_acc: 0.5288\n",
      "Epoch 63/250\n",
      "156/156 [==============================] - 53s 337ms/step - loss: 0.8890 - acc: 0.5905 - val_loss: 0.9392 - val_acc: 0.5361\n",
      "Epoch 64/250\n",
      "156/156 [==============================] - 52s 333ms/step - loss: 0.8872 - acc: 0.5889 - val_loss: 0.8926 - val_acc: 0.5689\n",
      "Epoch 65/250\n",
      "156/156 [==============================] - 54s 345ms/step - loss: 0.9108 - acc: 0.5653 - val_loss: 0.9259 - val_acc: 0.5457\n",
      "Epoch 66/250\n",
      "156/156 [==============================] - 53s 343ms/step - loss: 0.8931 - acc: 0.5717 - val_loss: 0.9129 - val_acc: 0.5401\n",
      "Epoch 67/250\n",
      "156/156 [==============================] - 51s 327ms/step - loss: 0.8878 - acc: 0.5701 - val_loss: 0.8989 - val_acc: 0.5601\n",
      "Epoch 68/250\n",
      "156/156 [==============================] - 61s 393ms/step - loss: 0.9072 - acc: 0.5729 - val_loss: 0.9165 - val_acc: 0.5385\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 0.421875.\n",
      "Epoch 69/250\n",
      "156/156 [==============================] - 51s 327ms/step - loss: 0.8730 - acc: 0.5961 - val_loss: 0.9076 - val_acc: 0.5697\n",
      "Epoch 70/250\n",
      "156/156 [==============================] - 52s 333ms/step - loss: 0.8766 - acc: 0.5946 - val_loss: 0.9011 - val_acc: 0.5337\n",
      "Epoch 71/250\n",
      "156/156 [==============================] - 53s 341ms/step - loss: 0.8802 - acc: 0.5833 - val_loss: 0.9093 - val_acc: 0.5545\n",
      "Epoch 72/250\n",
      "156/156 [==============================] - 53s 342ms/step - loss: 0.8785 - acc: 0.5913 - val_loss: 0.9139 - val_acc: 0.5673\n",
      "Epoch 73/250\n",
      "156/156 [==============================] - 62s 396ms/step - loss: 0.8888 - acc: 0.5777 - val_loss: 0.9198 - val_acc: 0.5529\n",
      "Epoch 74/250\n",
      "156/156 [==============================] - 53s 342ms/step - loss: 0.8747 - acc: 0.6022 - val_loss: 0.9390 - val_acc: 0.5321\n",
      "Epoch 75/250\n",
      "156/156 [==============================] - 54s 344ms/step - loss: 0.8946 - acc: 0.5661 - val_loss: 0.9037 - val_acc: 0.5601\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 0.31640625.\n",
      "Epoch 76/250\n",
      "156/156 [==============================] - 54s 346ms/step - loss: 0.8552 - acc: 0.6018 - val_loss: 0.8655 - val_acc: 0.5689\n",
      "Epoch 77/250\n",
      "156/156 [==============================] - 60s 387ms/step - loss: 0.8598 - acc: 0.5860 - val_loss: 0.9678 - val_acc: 0.5513\n",
      "Epoch 78/250\n",
      "156/156 [==============================] - 53s 342ms/step - loss: 0.8978 - acc: 0.5585 - val_loss: 0.8966 - val_acc: 0.5481\n",
      "Epoch 79/250\n",
      "156/156 [==============================] - 53s 340ms/step - loss: 0.8604 - acc: 0.5913 - val_loss: 0.8723 - val_acc: 0.5865\n",
      "Epoch 80/250\n",
      "156/156 [==============================] - 55s 350ms/step - loss: 0.8423 - acc: 0.6078 - val_loss: 0.9073 - val_acc: 0.5681\n",
      "Epoch 81/250\n",
      "156/156 [==============================] - 52s 333ms/step - loss: 0.8621 - acc: 0.5913 - val_loss: 0.9397 - val_acc: 0.5433\n",
      "Epoch 82/250\n",
      "156/156 [==============================] - 57s 363ms/step - loss: 0.8626 - acc: 0.5929 - val_loss: 0.9108 - val_acc: 0.5873\n",
      "Epoch 83/250\n",
      "156/156 [==============================] - 51s 326ms/step - loss: 0.8616 - acc: 0.6110 - val_loss: 0.8772 - val_acc: 0.5617\n",
      "Epoch 84/250\n",
      "156/156 [==============================] - 56s 356ms/step - loss: 0.8648 - acc: 0.6015 - val_loss: 0.9301 - val_acc: 0.5673\n",
      "Epoch 85/250\n",
      "156/156 [==============================] - 53s 338ms/step - loss: 0.8674 - acc: 0.5950 - val_loss: 0.8993 - val_acc: 0.5625\n",
      "Epoch 86/250\n",
      "156/156 [==============================] - 53s 337ms/step - loss: 0.8687 - acc: 0.6034 - val_loss: 0.8897 - val_acc: 0.5753\n",
      "Epoch 87/250\n",
      "156/156 [==============================] - 52s 336ms/step - loss: 0.8719 - acc: 0.6014 - val_loss: 0.9300 - val_acc: 0.5529\n",
      "Epoch 88/250\n",
      "156/156 [==============================] - 54s 349ms/step - loss: 0.8586 - acc: 0.5962 - val_loss: 0.8792 - val_acc: 0.5665\n",
      "Epoch 89/250\n",
      "156/156 [==============================] - 54s 346ms/step - loss: 0.8464 - acc: 0.6102 - val_loss: 0.9018 - val_acc: 0.5697\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 0.2373046875.\n",
      "Epoch 90/250\n",
      "156/156 [==============================] - 54s 345ms/step - loss: 0.8558 - acc: 0.5998 - val_loss: 0.8893 - val_acc: 0.5946\n",
      "Epoch 91/250\n",
      "156/156 [==============================] - 53s 340ms/step - loss: 0.8487 - acc: 0.6018 - val_loss: 0.8788 - val_acc: 0.5809\n",
      "Epoch 92/250\n",
      "156/156 [==============================] - 57s 365ms/step - loss: 0.8398 - acc: 0.5994 - val_loss: 0.8919 - val_acc: 0.5569\n",
      "Epoch 93/250\n",
      "156/156 [==============================] - 51s 328ms/step - loss: 0.8513 - acc: 0.6166 - val_loss: 0.8797 - val_acc: 0.5857\n",
      "Epoch 94/250\n",
      "156/156 [==============================] - 57s 365ms/step - loss: 0.8572 - acc: 0.6102 - val_loss: 0.8995 - val_acc: 0.5817\n",
      "Epoch 95/250\n",
      "156/156 [==============================] - 53s 342ms/step - loss: 0.8197 - acc: 0.6286 - val_loss: 0.8760 - val_acc: 0.5825\n",
      "Epoch 96/250\n",
      "156/156 [==============================] - 52s 332ms/step - loss: 0.8394 - acc: 0.6194 - val_loss: 0.8933 - val_acc: 0.5849\n",
      "Epoch 00096: early stopping\n"
     ]
    }
   ],
   "source": [
    "#change 2500 to entire size of dataset (21123)\n",
    "model.fit_generator(train_generator, steps_per_epoch = 2500 // batch_size, epochs = 250, validation_data =v_generator, validation_steps = 1250 // batch_size, callbacks = [board, e_stop, reduce_lr], workers = 7)\n",
    "model.save_weights('artsnobs_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
