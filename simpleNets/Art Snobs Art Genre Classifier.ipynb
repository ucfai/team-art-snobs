{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import keras\n",
    "board = keras.callbacks.TensorBoard(log_dir='./logs/artsnobs', histogram_freq=0, batch_size=100, write_graph=True)\n",
    "\n",
    "#Dynamically change learning rate based on 'val_acc' over time\n",
    "#change 0.8 to 0.75?\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.8, patience=7, min_lr=0.001, verbose=1)\n",
    "\n",
    "#Stop training when 'val_loss' stops improving after 15 epochs\n",
    "e_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1, mode='auto')\n",
    "\n",
    "#Next 4 lines allows MASSIVE images to be loaded in and used\n",
    "import PIL\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "PIL.Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3)))\n",
    "# model.add(Conv2D(32, (3, 3), input_shape=(175, 175, 3)))\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(500, 500, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "# model.add(BatchNormalization()) #added this\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3))) #changed 32 to 64\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3))) #added this\n",
    "model.add(Activation('relu')) #added this\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) #added this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# model.add(Dense(64)) #added this\n",
    "# model.add(Activation('sigmoid')) #added this\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3)) #3 for art_genre_set, 5 for dataset\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adadelta', metrics = ['accuracy'])\n",
    "# model.summary()\n",
    "# model.load_weights('artsnobs_weights.h5') #BE AWARE IF THIS IS ENABLED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 #change this from 16, used 100 for dataset\n",
    "# train_datagen = ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255, rotation_range=40, width_shift_range=0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True, fill_mode = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19011 images belonging to 3 classes.\n",
      "Found 2112 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "train_generator = train_datagen.flow_from_directory('/Users/ethan/Documents/MLData/ArtSnobs/art_genre_set/train/', target_size = (500, 500), batch_size = batch_size, class_mode = 'categorical', color_mode='rgb')\n",
    "v_generator = test_datagen.flow_from_directory('/Users/ethan/Documents/MLData/ArtSnobs/art_genre_set/test/', target_size = (500, 500), batch_size = batch_size, class_mode = 'categorical', color_mode='rgb')\n",
    "\n",
    "# train_generator = train_datagen.flow_from_directory('/Users/ethan/Documents/MLData/ArtSnobs/dataset/dataset_updated/training_set/', target_size = (175, 175), batch_size = batch_size, class_mode = 'categorical', color_mode='rgb')\n",
    "# v_generator = test_datagen.flow_from_directory('/Users/ethan/Documents/MLData/ArtSnobs/dataset/dataset_updated/validation_set/', target_size = (175,175), batch_size = batch_size, class_mode = 'categorical', color_mode='rgb')\n",
    "# # train_generator = train_datagen.flow_from_directory('/Users/ethan/Documents/MLData/ArtSnobs/dataset/dataset_updated/training_set/', target_size = (150, 150), batch_size = batch_size, class_mode = 'categorical', color_mode='rgb')\n",
    "# # v_generator = test_datagen.flow_from_directory('/Users/ethan/Documents/MLData/ArtSnobs/dataset/dataset_updated/validation_set/', target_size = (150,150), batch_size = batch_size, class_mode = 'categorical', color_mode='rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ethan\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/250\n",
      "156/156 [==============================] - 33s 210ms/step - loss: 1.1012 - acc: 0.3494 - val_loss: 1.0958 - val_acc: 0.4062\n",
      "Epoch 2/250\n",
      "156/156 [==============================] - 32s 204ms/step - loss: 1.0853 - acc: 0.3718 - val_loss: 1.0886 - val_acc: 0.3678\n",
      "Epoch 3/250\n",
      "156/156 [==============================] - 52s 334ms/step - loss: 1.0749 - acc: 0.4147 - val_loss: 1.0917 - val_acc: 0.3998\n",
      "Epoch 4/250\n",
      "156/156 [==============================] - 28s 183ms/step - loss: 1.0716 - acc: 0.4259 - val_loss: 1.0915 - val_acc: 0.4030\n",
      "Epoch 5/250\n",
      "156/156 [==============================] - 32s 203ms/step - loss: 1.0541 - acc: 0.4383 - val_loss: 1.0520 - val_acc: 0.4615\n",
      "Epoch 6/250\n",
      "156/156 [==============================] - 30s 190ms/step - loss: 1.0563 - acc: 0.4295 - val_loss: 1.0500 - val_acc: 0.4647\n",
      "Epoch 7/250\n",
      "156/156 [==============================] - 28s 182ms/step - loss: 1.0373 - acc: 0.4479 - val_loss: 1.0174 - val_acc: 0.4776\n",
      "Epoch 8/250\n",
      "156/156 [==============================] - 28s 181ms/step - loss: 1.0259 - acc: 0.4626 - val_loss: 1.0562 - val_acc: 0.4575\n",
      "Epoch 9/250\n",
      "156/156 [==============================] - 26s 168ms/step - loss: 1.0228 - acc: 0.4459 - val_loss: 1.0214 - val_acc: 0.4752\n",
      "Epoch 10/250\n",
      "156/156 [==============================] - 29s 185ms/step - loss: 1.0268 - acc: 0.4547 - val_loss: 0.9939 - val_acc: 0.4944\n",
      "Epoch 11/250\n",
      "156/156 [==============================] - 31s 199ms/step - loss: 1.0077 - acc: 0.4732 - val_loss: 1.0175 - val_acc: 0.4752\n",
      "Epoch 12/250\n",
      "156/156 [==============================] - 27s 174ms/step - loss: 1.0037 - acc: 0.4824 - val_loss: 1.0129 - val_acc: 0.4663\n",
      "Epoch 13/250\n",
      "156/156 [==============================] - 28s 180ms/step - loss: 0.9811 - acc: 0.4996 - val_loss: 0.9899 - val_acc: 0.5056\n",
      "Epoch 14/250\n",
      "156/156 [==============================] - 29s 184ms/step - loss: 1.0027 - acc: 0.4960 - val_loss: 0.9948 - val_acc: 0.4920\n",
      "Epoch 15/250\n",
      "156/156 [==============================] - 39s 249ms/step - loss: 1.0005 - acc: 0.5028 - val_loss: 0.9872 - val_acc: 0.4992\n",
      "Epoch 16/250\n",
      "156/156 [==============================] - 29s 185ms/step - loss: 0.9897 - acc: 0.5115 - val_loss: 0.9903 - val_acc: 0.5064\n",
      "Epoch 17/250\n",
      "156/156 [==============================] - 36s 230ms/step - loss: 0.9796 - acc: 0.5056 - val_loss: 0.9937 - val_acc: 0.5136\n",
      "Epoch 18/250\n",
      "156/156 [==============================] - 28s 181ms/step - loss: 0.9889 - acc: 0.5080 - val_loss: 0.9940 - val_acc: 0.5064\n",
      "Epoch 19/250\n",
      "156/156 [==============================] - 27s 176ms/step - loss: 0.9923 - acc: 0.4924 - val_loss: 0.9818 - val_acc: 0.5200\n",
      "Epoch 20/250\n",
      "156/156 [==============================] - 28s 181ms/step - loss: 0.9617 - acc: 0.5212 - val_loss: 1.0108 - val_acc: 0.4872\n",
      "Epoch 21/250\n",
      "156/156 [==============================] - 28s 177ms/step - loss: 0.9834 - acc: 0.4960 - val_loss: 0.9922 - val_acc: 0.4704\n",
      "Epoch 22/250\n",
      "156/156 [==============================] - 27s 173ms/step - loss: 0.9674 - acc: 0.5100 - val_loss: 0.9861 - val_acc: 0.4936\n",
      "Epoch 23/250\n",
      "156/156 [==============================] - 27s 173ms/step - loss: 0.9789 - acc: 0.5183 - val_loss: 0.9737 - val_acc: 0.5024\n",
      "Epoch 24/250\n",
      "156/156 [==============================] - 28s 178ms/step - loss: 0.9853 - acc: 0.5056 - val_loss: 0.9811 - val_acc: 0.5136\n",
      "Epoch 25/250\n",
      "156/156 [==============================] - 27s 170ms/step - loss: 0.9724 - acc: 0.5036 - val_loss: 0.9944 - val_acc: 0.5032\n",
      "Epoch 26/250\n",
      "156/156 [==============================] - 30s 191ms/step - loss: 0.9768 - acc: 0.5108 - val_loss: 0.9774 - val_acc: 0.4880\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.8.\n",
      "Epoch 27/250\n",
      "156/156 [==============================] - 29s 184ms/step - loss: 0.9630 - acc: 0.5244 - val_loss: 1.0090 - val_acc: 0.4952\n",
      "Epoch 28/250\n",
      "156/156 [==============================] - 27s 176ms/step - loss: 0.9588 - acc: 0.5381 - val_loss: 0.9982 - val_acc: 0.5136\n",
      "Epoch 29/250\n",
      "156/156 [==============================] - 27s 175ms/step - loss: 0.9566 - acc: 0.5236 - val_loss: 1.0703 - val_acc: 0.4543\n",
      "Epoch 30/250\n",
      "156/156 [==============================] - 35s 223ms/step - loss: 0.9564 - acc: 0.5365 - val_loss: 0.9473 - val_acc: 0.5264\n",
      "Epoch 31/250\n",
      "156/156 [==============================] - 26s 165ms/step - loss: 0.9329 - acc: 0.5435 - val_loss: 0.9670 - val_acc: 0.5104\n",
      "Epoch 32/250\n",
      "156/156 [==============================] - 27s 171ms/step - loss: 0.9633 - acc: 0.5136 - val_loss: 1.0143 - val_acc: 0.4888\n",
      "Epoch 33/250\n",
      "156/156 [==============================] - 29s 184ms/step - loss: 0.9483 - acc: 0.5385 - val_loss: 0.9840 - val_acc: 0.5304\n",
      "Epoch 34/250\n",
      "156/156 [==============================] - 27s 176ms/step - loss: 0.9454 - acc: 0.5276 - val_loss: 1.0176 - val_acc: 0.4936\n",
      "Epoch 35/250\n",
      "156/156 [==============================] - 26s 168ms/step - loss: 0.9488 - acc: 0.5264 - val_loss: 0.9637 - val_acc: 0.4984\n",
      "Epoch 36/250\n",
      "156/156 [==============================] - 26s 166ms/step - loss: 0.9495 - acc: 0.5212 - val_loss: 0.9802 - val_acc: 0.5296\n",
      "Epoch 37/250\n",
      "156/156 [==============================] - 36s 232ms/step - loss: 0.9175 - acc: 0.5569 - val_loss: 0.9574 - val_acc: 0.5080\n",
      "Epoch 38/250\n",
      "156/156 [==============================] - 28s 180ms/step - loss: 0.9657 - acc: 0.5144 - val_loss: 0.9791 - val_acc: 0.5056\n",
      "Epoch 39/250\n",
      "156/156 [==============================] - 26s 164ms/step - loss: 0.9484 - acc: 0.5327 - val_loss: 0.9616 - val_acc: 0.5128\n",
      "Epoch 40/250\n",
      "156/156 [==============================] - 26s 167ms/step - loss: 0.9413 - acc: 0.5349 - val_loss: 0.9816 - val_acc: 0.5088\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.6400000095367432.\n",
      "Epoch 41/250\n",
      "156/156 [==============================] - 27s 172ms/step - loss: 0.9270 - acc: 0.5485 - val_loss: 0.9420 - val_acc: 0.5224\n",
      "Epoch 42/250\n",
      "156/156 [==============================] - 37s 240ms/step - loss: 0.9222 - acc: 0.5569 - val_loss: 0.9379 - val_acc: 0.5409\n",
      "Epoch 43/250\n",
      "156/156 [==============================] - 29s 183ms/step - loss: 0.9232 - acc: 0.5565 - val_loss: 0.9477 - val_acc: 0.5064\n",
      "Epoch 44/250\n",
      "156/156 [==============================] - 26s 165ms/step - loss: 0.9336 - acc: 0.5633 - val_loss: 0.9663 - val_acc: 0.5208\n",
      "Epoch 45/250\n",
      "156/156 [==============================] - 27s 173ms/step - loss: 0.9157 - acc: 0.5573 - val_loss: 0.9645 - val_acc: 0.5345\n",
      "Epoch 46/250\n",
      "156/156 [==============================] - 28s 180ms/step - loss: 0.9390 - acc: 0.5470 - val_loss: 0.9497 - val_acc: 0.5192\n",
      "Epoch 47/250\n",
      "156/156 [==============================] - 33s 214ms/step - loss: 0.9188 - acc: 0.5481 - val_loss: 0.9412 - val_acc: 0.5272\n",
      "Epoch 48/250\n",
      "156/156 [==============================] - 26s 165ms/step - loss: 0.9201 - acc: 0.5501 - val_loss: 0.9414 - val_acc: 0.5272\n",
      "Epoch 49/250\n",
      "156/156 [==============================] - 25s 163ms/step - loss: 0.9155 - acc: 0.5529 - val_loss: 0.9623 - val_acc: 0.5393\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.5119999885559082.\n",
      "Epoch 50/250\n",
      "156/156 [==============================] - 27s 174ms/step - loss: 0.9116 - acc: 0.5749 - val_loss: 0.9298 - val_acc: 0.5465\n",
      "Epoch 51/250\n",
      "156/156 [==============================] - 30s 190ms/step - loss: 0.9226 - acc: 0.5629 - val_loss: 0.9390 - val_acc: 0.5385\n",
      "Epoch 52/250\n",
      "156/156 [==============================] - 26s 170ms/step - loss: 0.9154 - acc: 0.5553 - val_loss: 0.9314 - val_acc: 0.5465\n",
      "Epoch 53/250\n",
      "156/156 [==============================] - 29s 184ms/step - loss: 0.9005 - acc: 0.5785 - val_loss: 0.9240 - val_acc: 0.5433\n",
      "Epoch 54/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 26s 169ms/step - loss: 0.9218 - acc: 0.5466 - val_loss: 0.9271 - val_acc: 0.5409\n",
      "Epoch 55/250\n",
      "156/156 [==============================] - 26s 165ms/step - loss: 0.9063 - acc: 0.5713 - val_loss: 0.9804 - val_acc: 0.5441\n",
      "Epoch 56/250\n",
      "156/156 [==============================] - 27s 175ms/step - loss: 0.9160 - acc: 0.5797 - val_loss: 0.9545 - val_acc: 0.5208\n",
      "Epoch 57/250\n",
      "156/156 [==============================] - 34s 219ms/step - loss: 0.9183 - acc: 0.5689 - val_loss: 0.9331 - val_acc: 0.5457\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.40959997177124025.\n",
      "Epoch 58/250\n",
      "156/156 [==============================] - 26s 165ms/step - loss: 0.8945 - acc: 0.5673 - val_loss: 0.9310 - val_acc: 0.5521\n",
      "Epoch 59/250\n",
      "156/156 [==============================] - 27s 171ms/step - loss: 0.9059 - acc: 0.5753 - val_loss: 0.9227 - val_acc: 0.5433\n",
      "Epoch 60/250\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.8791 - acc: 0.5921 - val_loss: 0.9372 - val_acc: 0.5216\n",
      "Epoch 61/250\n",
      "156/156 [==============================] - 27s 175ms/step - loss: 0.9093 - acc: 0.5719 - val_loss: 0.9191 - val_acc: 0.5345\n",
      "Epoch 62/250\n",
      "156/156 [==============================] - 35s 226ms/step - loss: 0.8991 - acc: 0.5701 - val_loss: 0.9479 - val_acc: 0.5321\n",
      "Epoch 63/250\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.8912 - acc: 0.5781 - val_loss: 0.9166 - val_acc: 0.5288\n",
      "Epoch 64/250\n",
      "156/156 [==============================] - 27s 172ms/step - loss: 0.8778 - acc: 0.5849 - val_loss: 0.9610 - val_acc: 0.5385\n",
      "Epoch 65/250\n",
      "156/156 [==============================] - 25s 162ms/step - loss: 0.8858 - acc: 0.5849 - val_loss: 0.9127 - val_acc: 0.5585\n",
      "Epoch 66/250\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.8915 - acc: 0.5693 - val_loss: 0.9647 - val_acc: 0.5449\n",
      "Epoch 67/250\n",
      "156/156 [==============================] - 26s 164ms/step - loss: 0.8971 - acc: 0.5693 - val_loss: 0.9188 - val_acc: 0.5505\n",
      "Epoch 68/250\n",
      "156/156 [==============================] - 26s 170ms/step - loss: 0.8960 - acc: 0.5721 - val_loss: 0.9276 - val_acc: 0.5401\n",
      "Epoch 69/250\n",
      "156/156 [==============================] - 27s 175ms/step - loss: 0.9111 - acc: 0.5626 - val_loss: 0.9185 - val_acc: 0.5393\n",
      "Epoch 70/250\n",
      "156/156 [==============================] - 28s 182ms/step - loss: 0.8860 - acc: 0.5817 - val_loss: 0.9241 - val_acc: 0.5505\n",
      "Epoch 71/250\n",
      "156/156 [==============================] - 34s 218ms/step - loss: 0.8854 - acc: 0.5769 - val_loss: 0.9954 - val_acc: 0.5329\n",
      "Epoch 72/250\n",
      "156/156 [==============================] - 25s 163ms/step - loss: 0.8894 - acc: 0.5725 - val_loss: 0.9141 - val_acc: 0.5665\n",
      "Epoch 73/250\n",
      "156/156 [==============================] - 31s 200ms/step - loss: 0.8816 - acc: 0.5833 - val_loss: 0.9389 - val_acc: 0.5409\n",
      "Epoch 74/250\n",
      "156/156 [==============================] - 27s 170ms/step - loss: 0.8843 - acc: 0.5857 - val_loss: 0.9521 - val_acc: 0.5441\n",
      "Epoch 75/250\n",
      "156/156 [==============================] - 25s 162ms/step - loss: 0.8754 - acc: 0.5933 - val_loss: 0.9555 - val_acc: 0.5345\n",
      "Epoch 76/250\n",
      "156/156 [==============================] - 25s 162ms/step - loss: 0.8938 - acc: 0.5745 - val_loss: 0.9197 - val_acc: 0.5545\n",
      "Epoch 77/250\n",
      "156/156 [==============================] - 25s 163ms/step - loss: 0.8970 - acc: 0.5625 - val_loss: 0.8841 - val_acc: 0.5761\n",
      "Epoch 78/250\n",
      "156/156 [==============================] - 25s 163ms/step - loss: 0.8785 - acc: 0.6038 - val_loss: 0.9809 - val_acc: 0.5056\n",
      "Epoch 79/250\n",
      "156/156 [==============================] - 27s 172ms/step - loss: 0.8616 - acc: 0.5978 - val_loss: 0.9374 - val_acc: 0.5521\n",
      "Epoch 80/250\n",
      "156/156 [==============================] - 27s 175ms/step - loss: 0.9061 - acc: 0.5593 - val_loss: 0.9217 - val_acc: 0.5337\n",
      "Epoch 81/250\n",
      "156/156 [==============================] - 35s 223ms/step - loss: 0.8907 - acc: 0.5897 - val_loss: 0.9207 - val_acc: 0.5369\n",
      "Epoch 82/250\n",
      "156/156 [==============================] - 28s 176ms/step - loss: 0.8674 - acc: 0.5970 - val_loss: 0.9296 - val_acc: 0.5505\n",
      "Epoch 83/250\n",
      "156/156 [==============================] - 26s 167ms/step - loss: 0.8805 - acc: 0.5781 - val_loss: 0.9240 - val_acc: 0.5449\n",
      "Epoch 84/250\n",
      "156/156 [==============================] - 26s 165ms/step - loss: 0.8790 - acc: 0.5836 - val_loss: 0.9567 - val_acc: 0.5377\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 0.32767996788024906.\n",
      "Epoch 85/250\n",
      "156/156 [==============================] - 35s 225ms/step - loss: 0.8715 - acc: 0.5925 - val_loss: 0.9393 - val_acc: 0.5417\n",
      "Epoch 86/250\n",
      "156/156 [==============================] - 26s 169ms/step - loss: 0.8667 - acc: 0.5913 - val_loss: 0.9635 - val_acc: 0.5425\n",
      "Epoch 87/250\n",
      "156/156 [==============================] - 27s 170ms/step - loss: 0.8799 - acc: 0.5857 - val_loss: 0.8956 - val_acc: 0.5633\n",
      "Epoch 88/250\n",
      "156/156 [==============================] - 27s 173ms/step - loss: 0.8730 - acc: 0.5905 - val_loss: 0.9471 - val_acc: 0.5321\n",
      "Epoch 89/250\n",
      "156/156 [==============================] - 27s 173ms/step - loss: 0.8771 - acc: 0.5869 - val_loss: 0.9195 - val_acc: 0.5417\n",
      "Epoch 90/250\n",
      "156/156 [==============================] - 26s 167ms/step - loss: 0.8858 - acc: 0.5849 - val_loss: 0.9073 - val_acc: 0.5609\n",
      "Epoch 91/250\n",
      "156/156 [==============================] - 26s 169ms/step - loss: 0.8757 - acc: 0.6034 - val_loss: 0.9303 - val_acc: 0.5481\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 0.26214396953582764.\n",
      "Epoch 92/250\n",
      "156/156 [==============================] - 27s 173ms/step - loss: 0.8642 - acc: 0.5888 - val_loss: 0.9408 - val_acc: 0.5569\n",
      "Epoch 00092: early stopping\n"
     ]
    }
   ],
   "source": [
    "#change 2500 to entire size of dataset (21123)\n",
    "model.fit_generator(train_generator, steps_per_epoch = 2500 // batch_size, epochs = 250, validation_data =v_generator, validation_steps = 1250 // batch_size, callbacks = [board, e_stop, reduce_lr], workers = 7)\n",
    "model.save_weights('artsnobs_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
